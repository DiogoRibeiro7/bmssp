#!/usr/bin/env python3
"""
Expanded BMSSP Test Suite

Comprehensive testing framework for BMSSP implementations including:
- Edge cases (disconnected graphs, single nodes, empty graphs)
- Property-based testing with graph invariants
- Regression tests for algorithmic behavior
- Cross-language consistency verification
- Performance regression detection
"""

import os
import sys
import json
import subprocess
import time
import random
import math
import networkx as nx
import pytest
import hypothesis
from hypothesis import strategies as st, given, settings, assume
from typing import Dict, List, Tuple, Set, Optional, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import tempfile
import shutil
from collections import defaultdict
import numpy as np

@dataclass
class TestResult:
    """Container for test execution results."""
    implementation: str
    test_case: str
    distances: Dict[str, float]
    execution_time: float
    memory_usage: float
    vertices_processed: int
    error: Optional[str] = None
    metadata: Dict[str, Any] = None

@dataclass
class GraphTestCase:
    """Test case specification with expected properties."""
    name: str
    graph_data: Dict[str, Any]  # Serializable graph representation
    source: str
    expected_properties: Dict[str, Any]
    description: str
    category: str

class GraphGenerator:
    """Generates various types of test graphs with known properties."""
    
    @staticmethod
    def empty_graph() -> nx.DiGraph:
        """Empty graph with no vertices."""
        return nx.DiGraph()
    
    @staticmethod
    def single_vertex() -> nx.DiGraph:
        """Single vertex with no edges."""
        G = nx.DiGraph()
        G.add_node('A')
        return G
    
    @staticmethod
    def two_disconnected_vertices() -> nx.DiGraph:
        """Two vertices with no connection."""
        G = nx.DiGraph()
        G.add_nodes_from(['A', 'B'])
        return G
    
    @staticmethod
    def disconnected_components() -> nx.DiGraph:
        """Multiple disconnected components."""
        G = nx.DiGraph()
        # Component 1: A -> B -> C
        G.add_edge('A', 'B', weight=1)
        G.add_edge('B', 'C', weight=2)
        # Component 2: D -> E (disconnected)
        G.add_edge('D', 'E', weight=3)
        # Component 3: F (isolated)
        G.add_node('F')
        return G
    
    @staticmethod
    def self_loop() -> nx.DiGraph:
        """Graph with self-loops."""
        G = nx.DiGraph()
        G.add_edge('A', 'A', weight=1)
        G.add_edge('A', 'B', weight=2)
        G.add_edge('B', 'B', weight=3)
        return G
    
    @staticmethod
    def negative_zero_weights() -> nx.DiGraph:
        """Graph with zero and very small weights."""
        G = nx.DiGraph()
        G.add_edge('A', 'B', weight=0)
        G.add_edge('B', 'C', weight=0.000001)
        G.add_edge('A', 'C', weight=0.000002)
        return G
    
    @staticmethod
    def large_weights() -> nx.DiGraph:
        """Graph with very large weights."""
        G = nx.DiGraph()
        G.add_edge('A', 'B', weight=1e10)
        G.add_edge('B', 'C', weight=1e10)
        G.add_edge('A', 'C', weight=2e10 - 1)  # Should be optimal
        return G
    
    @staticmethod
    def star_graph(n: int) -> nx.DiGraph:
        """Star graph with center and n-1 leaves."""
        G = nx.DiGraph()
        center = 'center'
        for i in range(n-1):
            leaf = f'leaf_{i}'
            G.add_edge(center, leaf, weight=i+1)
        return G
    
    @staticmethod
    def complete_graph(n: int) -> nx.DiGraph:
        """Complete directed graph."""
        G = nx.DiGraph()
        vertices = [f'v{i}' for i in range(n)]
        for i, u in enumerate(vertices):
            for j, v in enumerate(vertices):
                if i != j:
                    G.add_edge(u, v, weight=abs(i-j))
        return G
    
    @staticmethod
    def long_path(n: int) -> nx.DiGraph:
        """Long path graph."""
        G = nx.DiGraph()
        vertices = [f'v{i}' for i in range(n)]
        for i in range(n-1):
            G.add_edge(vertices[i], vertices[i+1], weight=1)
        return G
    
    @staticmethod
    def binary_tree(depth: int) -> nx.DiGraph:
        """Complete binary tree."""
        G = nx.DiGraph()
        
        def add_tree(node_id, current_depth):
            if current_depth >= depth:
                return
            left = f'{node_id}_L'
            right = f'{node_id}_R'
            G.add_edge(node_id, left, weight=1)
            G.add_edge(node_id, right, weight=1)
            add_tree(left, current_depth + 1)
            add_tree(right, current_depth + 1)
        
        add_tree('root', 0)
        return G

class PropertyTester:
    """Property-based testing for graph algorithms."""
    
    @staticmethod
    def verify_distance_properties(distances: Dict[str, float], 
                                 graph: nx.DiGraph, 
                                 source: str) -> List[str]:
        """Verify basic distance properties."""
        violations = []
        
        # Property 1: Source distance should be 0
        if source in distances and distances[source] != 0:
            violations.append(f"Source {source} distance is {distances[source]}, not 0")
        
        # Property 2: All distances should be non-negative
        for vertex, dist in distances.items():
            if dist < 0 and dist != float('inf'):
                violations.append(f"Negative distance {dist} for vertex {vertex}")
        
        # Property 3: Triangle inequality (for reachable vertices)
        for u in graph.nodes():
            if u not in distances or distances[u] == float('inf'):
                continue
            for v in graph.neighbors(u):
                if v in distances and distances[v] != float('inf'):
                    edge_weight = graph[u][v]['weight']
                    if distances[v] > distances[u] + edge_weight + 1e-10:  # Small epsilon for floating point
                        violations.append(
                            f"Triangle inequality violated: {u}->{v}, "
                            f"dist({v})={distances[v]} > dist({u})+weight = {distances[u]}+{edge_weight}"
                        )
        
        # Property 4: Unreachable vertices should have infinite distance
        reachable = set()
        if source in graph.nodes():
            try:
                reachable = set(nx.single_source_shortest_path_length(graph, source).keys())
            except:
                pass
        
        for vertex in graph.nodes():
            if vertex not in reachable and vertex in distances:
                if distances[vertex] != float('inf'):
                    # Note: BMSSP might not reach all reachable vertices due to bounds
                    # This is expected behavior, not a violation
                    pass
        
        return violations
    
    @staticmethod
    def verify_algorithmic_invariants(result: TestResult, 
                                    graph: nx.DiGraph,
                                    source: str) -> List[str]:
        """Verify algorithm-specific invariants."""
        violations = []
        
        # BMSSP-specific properties
        if result.implementation.lower().startswith('bmssp'):
            # Property 1: Bounded exploration - some vertices might be unreachable
            # even if paths exist (this is expected behavior)
            
            # Property 2: Parameter consistency
            n = len(graph.nodes())
            if n > 0:
                expected_k = max(1, int(math.log(n, 2) ** (1/3)))
                expected_t = max(1, int(math.log(n, 2) ** (2/3)))
                expected_l = math.ceil(math.log(n, 2) / expected_t)
                
                # These are the expected parameters for theoretical analysis
                # Actual implementation might use different values
        
        # Dijkstra-specific properties
        elif result.implementation.lower().startswith('dijkstra'):
            # Property 1: Should find optimal distances to all reachable vertices
            try:
                expected_distances = nx.single_source_shortest_path_length(
                    graph, source, weight='weight'
                )
                for vertex, expected_dist in expected_distances.items():
                    if vertex in result.distances:
                        if abs(result.distances[vertex] - expected_dist) > 1e-10:
                            violations.append(
                                f"Dijkstra distance mismatch for {vertex}: "
                                f"got {result.distances[vertex]}, expected {expected_dist}"
                            )
            except Exception as e:
                # Graph might be invalid for NetworkX
                pass
        
        return violations

class EdgeCaseTests:
    """Comprehensive edge case testing."""
    
    def __init__(self):
        self.test_cases = self._generate_edge_cases()
    
    def _generate_edge_cases(self) -> List[GraphTestCase]:
        """Generate all edge case test scenarios."""
        cases = []
        
        # Empty graph
        cases.append(GraphTestCase(
            name="empty_graph",
            graph_data=self._serialize_graph(GraphGenerator.empty_graph()),
            source="nonexistent",
            expected_properties={
                "should_handle_gracefully": True,
                "expected_error": True
            },
            description="Empty graph with no vertices",
            category="edge_cases"
        ))
        
        # Single vertex
        single_graph = GraphGenerator.single_vertex()
        cases.append(GraphTestCase(
            name="single_vertex",
            graph_data=self._serialize_graph(single_graph),
            source="A",
            expected_properties={
                "reachable_count": 1,
                "source_distance": 0,
                "total_distance": 0
            },
            description="Single vertex with no edges",
            category="edge_cases"
        ))
        
        # Two disconnected vertices
        disconnected_graph = GraphGenerator.two_disconnected_vertices()
        cases.append(GraphTestCase(
            name="two_disconnected",
            graph_data=self._serialize_graph(disconnected_graph),
            source="A",
            expected_properties={
                "reachable_count": 1,  # Only source
                "unreachable_vertices": ["B"]
            },
            description="Two disconnected vertices",
            category="edge_cases"
        ))
        
        # Multiple components
        multi_component = GraphGenerator.disconnected_components()
        cases.append(GraphTestCase(
            name="disconnected_components",
            graph_data=self._serialize_graph(multi_component),
            source="A",
            expected_properties={
                "reachable_count": 3,  # A, B, C
                "unreachable_vertices": ["D", "E", "F"]
            },
            description="Multiple disconnected components",
            category="edge_cases"
        ))
        
        # Self-loops
        self_loop_graph = GraphGenerator.self_loop()
        cases.append(GraphTestCase(
            name="self_loops",
            graph_data=self._serialize_graph(self_loop_graph),
            source="A",
            expected_properties={
                "reachable_count": 2,  # A, B
                "handles_self_loops": True
            },
            description="Graph with self-loops",
            category="edge_cases"
        ))
        
        # Zero weights
        zero_weight_graph = GraphGenerator.negative_zero_weights()
        cases.append(GraphTestCase(
            name="zero_weights",
            graph_data=self._serialize_graph(zero_weight_graph),
            source="A",
            expected_properties={
                "handles_zero_weights": True,
                "reachable_count": 3
            },
            description="Graph with zero and very small weights",
            category="edge_cases"
        ))
        
        # Large weights
        large_weight_graph = GraphGenerator.large_weights()
        cases.append(GraphTestCase(
            name="large_weights",
            graph_data=self._serialize_graph(large_weight_graph),
            source="A",
            expected_properties={
                "handles_large_weights": True,
                "reachable_count": 3
            },
            description="Graph with very large weights",
            category="edge_cases"
        ))
        
        return cases
    
    def _serialize_graph(self, graph: nx.DiGraph) -> Dict[str, Any]:
        """Convert NetworkX graph to serializable format."""
        return {
            "nodes": list(graph.nodes()),
            "edges": [(u, v, graph[u][v].get('weight', 1)) 
                     for u, v in graph.edges()]
        }

class PropertyBasedTests:
    """Property-based testing using Hypothesis."""
    
    @staticmethod
    @given(
        n=st.integers(min_value=1, max_value=20),
        density=st.floats(min_value=0.1, max_value=1.0),
        weight_range=st.tuples(
            st.floats(min_value=0.1, max_value=10.0),
            st.floats(min_value=1.0, max_value=100.0)
        )
    )
    @settings(max_examples=50, deadline=10000)  # 10 second timeout
    def test_random_graph_properties(n: int, density: float, weight_range: Tuple[float, float]):
        """Test properties on randomly generated graphs."""
        assume(weight_range[0] < weight_range[1])
        
        # Generate random graph
        graph = nx.erdos_renyi_graph(n, density, directed=True)
        
        # Add weights
        for u, v in graph.edges():
            weight = random.uniform(weight_range[0], weight_range[1])
            graph[u][v]['weight'] = weight
        
        if len(graph.nodes()) == 0:
            return  # Skip empty graphs
        
        source = list(graph.nodes())[0]
        
        # Test with multiple implementations
        implementations = ['python_bmssp', 'dijkstra_reference']
        results = {}
        
        for impl in implementations:
            try:
                result = run_implementation(impl, graph, source)
                results[impl] = result
                
                # Verify basic properties
                violations = PropertyTester.verify_distance_properties(
                    result.distances, graph, source
                )
                assert len(violations) == 0, f"Property violations in {impl}: {violations}"
                
            except Exception as e:
                # Some implementations might fail on certain graphs
                # This is acceptable for BMSSP due to its bounded nature
                if 'bmssp' not in impl.lower():
                    raise  # Dijkstra should always work
    
    @staticmethod
    @given(
        tree_depth=st.integers(min_value=1, max_value=6)
    )
    @settings(max_examples=20)
    def test_tree_properties(tree_depth: int):
        """Test properties specific to tree structures."""
        graph = GraphGenerator.binary_tree(tree_depth)
        source = 'root'
        
        result = run_implementation('python_bmssp', graph, source)
        
        # Tree-specific properties
        # 1. All distances should be equal to tree distance
        # 2. No cycles should affect results
        
        violations = PropertyTester.verify_distance_properties(
            result.distances, graph, source
        )
        assert len(violations) == 0, f"Tree property violations: {violations}"
    
    @staticmethod
    @given(
        path_length=st.integers(min_value=2, max_value=50)
    )
    @settings(max_examples=20)
    def test_path_properties(path_length: int):
        """Test properties on path graphs."""
        graph = GraphGenerator.long_path(path_length)
        source = 'v0'
        
        result = run_implementation('python_bmssp', graph, source)
        
        # Path-specific properties
        # 1. Distance to v_i should be i (for unit weights)
        # 2. All vertices should be reachable in optimal case
        
        violations = PropertyTester.verify_distance_properties(
            result.distances, graph, source
        )
        assert len(violations) == 0, f"Path property violations: {violations}"

class RegressionTests:
    """Regression tests for algorithmic behavior."""
    
    def __init__(self):
        self.baseline_results = self._load_baseline_results()
        self.tolerance = 1e-10
    
    def _load_baseline_results(self) -> Dict[str, TestResult]:
        """Load baseline results from previous test runs."""
        baseline_path = Path(__file__).parent / "baseline_results.json"
        
        if baseline_path.exists():
            with open(baseline_path, 'r') as f:
                data = json.load(f)
                return {k: TestResult(**v) for k, v in data.items()}
        else:
            return {}
    
    def _save_baseline_results(self, results: Dict[str, TestResult]):
        """Save current results as new baseline."""
        baseline_path = Path(__file__).parent / "baseline_results.json"
        
        serializable_results = {
            k: asdict(v) for k, v in results.items()
        }
        
        with open(baseline_path, 'w') as f:
            json.dump(serializable_results, f, indent=2, default=str)
    
    def run_regression_tests(self) -> Dict[str, bool]:
        """Run regression tests against baseline."""
        test_graphs = [
            ("standard_test", self._create_standard_test_graph(), "s"),
            ("complex_test", self._create_complex_test_graph(), "start"),
            ("edge_case_test", GraphGenerator.disconnected_components(), "A")
        ]
        
        current_results = {}
        regression_status = {}
        
        for test_name, graph, source in test_graphs:
            try:
                result = run_implementation('python_bmssp', graph, source)
                current_results[test_name] = result
                
                # Compare with baseline
                if test_name in self.baseline_results:
                    baseline = self.baseline_results[test_name]
                    is_consistent = self._compare_results(result, baseline)
                    regression_status[test_name] = is_consistent
                else:
                    regression_status[test_name] = True  # No baseline to compare
                    
            except Exception as e:
                regression_status[test_name] = False
                print(f"Regression test failed for {test_name}: {e}")
        
        # Update baseline if all tests pass
        if all(regression_status.values()):
            self._save_baseline_results(current_results)
        
        return regression_status
    
    def _compare_results(self, current: TestResult, baseline: TestResult) -> bool:
        """Compare current results with baseline."""
        # Compare distances
        for vertex in set(current.distances.keys()) | set(baseline.distances.keys()):
            curr_dist = current.distances.get(vertex, float('inf'))
            base_dist = baseline.distances.get(vertex, float('inf'))
            
            if abs(curr_dist - base_dist) > self.tolerance:
                print(f"Distance regression for {vertex}: {curr_dist} vs {base_dist}")
                return False
        
        # Compare reachability counts
        curr_reachable = sum(1 for d in current.distances.values() if d < float('inf'))
        base_reachable = sum(1 for d in baseline.distances.values() if d < float('inf'))
        
        if curr_reachable != base_reachable:
            print(f"Reachability regression: {curr_reachable} vs {base_reachable}")
            return False
        
        return True
    
    def _create_standard_test_graph(self) -> nx.DiGraph:
        """Create standard test graph for regression testing."""
        G = nx.DiGraph()
        edges = [
            ('s', 'a', 1), ('s', 'b', 4),
            ('a', 'b', 2), ('a', 'c', 5),
            ('b', 'c', 1)
        ]
        for u, v, w in edges:
            G.add_edge(u, v, weight=w)
        return G
    
    def _create_complex_test_graph(self) -> nx.DiGraph:
        """Create more complex test graph."""
        G = nx.DiGraph()
        # Create a layered graph with cross connections
        layers = [['start'], ['a1', 'a2'], ['b1', 'b2', 'b3'], ['end']]
        
        # Add layer connections
        for i in range(len(layers) - 1):
            for u in layers[i]:
                for j, v in enumerate(layers[i + 1]):
                    G.add_edge(u, v, weight=i + j + 1)
        
        # Add cross connections
        G.add_edge('a1', 'b3', weight=2)
        G.add_edge('a2', 'end', weight=10)
        
        return G

class CrossLanguageTests:
    """Cross-language consistency verification."""
    
    def __init__(self):
        self.implementations = self._discover_implementations()
        self.test_tolerance = 1e-6
    
    def _discover_implementations(self) -> Dict[str, Dict[str, str]]:
        """Discover available language implementations."""
        repo_root = Path(__file__).parent.parent.parent
        implementations = {}
        
        # Python implementation
        python_path = repo_root / "implementations" / "python" / "bmssp.py"
        if python_path.exists():
            implementations['python'] = {
                'type': 'python',
                'path': str(python_path),
                'command': ['python', str(python_path)]
            }
        
        # Go implementation
        go_path = repo_root / "implementations" / "go" / "bmssp.go"
        if go_path.exists():
            implementations['go'] = {
                'type': 'go',
                'path': str(go_path),
                'command': ['go', 'run', str(go_path)]
            }
        
        # C implementation
        c_path = repo_root / "implementations" / "c" / "bmssp.c"
        if c_path.exists():
            implementations['c'] = {
                'type': 'c',
                'path': str(c_path),
                'command': None  # Will be compiled first
            }
        
        # Rust implementation
        rust_path = repo_root / "implementations" / "rust"
        if (rust_path / "Cargo.toml").exists():
            implementations['rust'] = {
                'type': 'rust',
                'path': str(rust_path),
                'command': ['cargo', 'run']
            }
        
        # Java implementation
        java_path = repo_root / "implementations" / "java" / "BMSSP.java"
        if java_path.exists():
            implementations['java'] = {
                'type': 'java',
                'path': str(java_path),
                'command': None  # Will be compiled first
            }
        
        return implementations
    
    def run_cross_language_tests(self) -> Dict[str, Dict[str, bool]]:
        """Run consistency tests across all available implementations."""
        test_cases = [
            ("simple_path", self._create_simple_test(), "A"),
            ("branching", self._create_branching_test(), "S"),
            ("disconnected", GraphGenerator.disconnected_components(), "A")
        ]
        
        results = {}
        
        for test_name, graph, source in test_cases:
            test_results = {}
            
            # Run all available implementations
            for lang, impl_info in self.implementations.items():
                try:
                    result = self._run_language_implementation(impl_info, graph, source)
                    test_results[lang] = result
                except Exception as e:
                    print(f"Failed to run {lang} implementation: {e}")
                    test_results[lang] = None
            
            # Compare results across languages
            consistency = self._check_cross_language_consistency(test_results)
            results[test_name] = consistency
        
        return results
    
    def _run_language_implementation(self, impl_info: Dict[str, str], 
                                   graph: nx.DiGraph, source: str) -> TestResult:
        """Run a specific language implementation."""
        # Create temporary input file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            graph_data = {
                'nodes': list(graph.nodes()),
                'edges': [(u, v, graph[u][v].get('weight', 1)) for u, v in graph.edges()],
                'source': source
            }
            json.dump(graph_data, f)
            input_file = f.name
        
        try:
            # Prepare command based on language
            if impl_info['type'] == 'python':
                cmd = ['python', impl_info['path'], input_file]
            elif impl_info['type'] == 'go':
                cmd = ['go', 'run', impl_info['path'], input_file]
            elif impl_info['type'] == 'c':
                # Compile first
                exe_path = impl_info['path'].replace('.c', '')
                compile_cmd = ['gcc', impl_info['path'], '-lm', '-o', exe_path]
                subprocess.run(compile_cmd, check=True, capture_output=True)
                cmd = [exe_path, input_file]
            elif impl_info['type'] == 'rust':
                cmd = ['cargo', 'run', '--', input_file]
                # Change to Rust directory
                original_cwd = os.getcwd()
                os.chdir(impl_info['path'])
            elif impl_info['type'] == 'java':
                # Compile first
                java_dir = Path(impl_info['path']).parent
                compile_cmd = ['javac', impl_info['path']]
                subprocess.run(compile_cmd, check=True, capture_output=True, cwd=java_dir)
                cmd = ['java', 'BMSSP', input_file]
                original_cwd = os.getcwd()
                os.chdir(java_dir)
            
            # Run implementation
            start_time = time.time()
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            execution_time = time.time() - start_time
            
            if impl_info['type'] in ['rust', 'java']:
                os.chdir(original_cwd)
            
            if result.returncode != 0:
                raise Exception(f"Implementation failed: {result.stderr}")
            
            # Parse output
            output_lines = result.stdout.strip().split('\n')
            distances = {}
            
            # Expected output format: "vertex distance" per line
            for line in output_lines:
                if line.strip():
                    parts = line.strip().split()
                    if len(parts) >= 2:
                        vertex = parts[0]
                        try:
                            dist = float(parts[1]) if parts[1] != 'inf' else float('inf')
                            distances[vertex] = dist
                        except ValueError:
                            continue
            
            return TestResult(
                implementation=impl_info['type'],
                test_case="cross_language",
                distances=distances,
                execution_time=execution_time,
                memory_usage=0,  # Not measured in cross-language tests
                vertices_processed=0
            )
            
        finally:
            # Clean up
            os.unlink(input_file)
    
    def _check_cross_language_consistency(self, results: Dict[str, Optional[TestResult]]) -> Dict[str, bool]:
        """Check consistency across language implementations."""
        consistency = {}
        
        # Filter out failed implementations
        valid_results = {k: v for k, v in results.items() if v is not None}
        
        if len(valid_results) < 2:
            return {"insufficient_implementations": False}
        
        # Compare each pair of implementations
        languages = list(valid_results.keys())
        
        for i in range(len(languages)):
            for j in range(i + 1, len(languages)):
                lang1, lang2 = languages[i], languages[j]
                result1, result2 = valid_results[lang1], valid_results[lang2]
                
                is_consistent = self._compare_language_results(result1, result2)
                consistency[f"{lang1}_vs_{lang2}"] = is_consistent
        
        return consistency
    
    def _compare_language_results(self, result1: TestResult, result2: TestResult) -> bool:
        """Compare results from two language implementations."""
        # Get all vertices mentioned in either result
        all_vertices = set(result1.distances.keys()) | set(result2.distances.keys())
        
        for vertex in all_vertices:
            dist1 = result1.distances.get(vertex, float('inf'))
            dist2 = result2.distances.get(vertex, float('inf'))
            
            # Handle infinite distances
            if math.isinf(dist1) and math.isinf(dist2):
                continue
            
            if math.isinf(dist1) or math.isinf(dist2):
                # One is infinite, the other is not
                print(f"Inconsistent reachability for {vertex}: {dist1} vs {dist2}")
                return False
            
            # Compare finite distances
            if abs(dist1 - dist2) > self.test_tolerance:
                print(f"Distance mismatch for {vertex}: {dist1} vs {dist2}")
                return False
        
        return True
    
    def _create_simple_test(self) -> nx.DiGraph:
        """Create simple test for cross-language verification."""
        G = nx.DiGraph()
        G.add_edge('A', 'B', weight=1)
        G.add_edge('B', 'C', weight=2)
        return G
    
    def _create_branching_test(self) -> nx.DiGraph:
        """Create branching test case."""
        G = nx.DiGraph()
        edges = [('S', 'A', 1), ('S', 'B', 3), ('A', 'C', 2), ('B', 'C', 1)]
        for u, v, w in edges:
            G.add_edge(u, v, weight=w)
        return G

class PerformanceRegressionTests:
    """Performance regression testing."""
    
    def __init__(self):
        self.baseline_performance = self._load_performance_baseline()
        self.performance_tolerance = 0.2  # 20% tolerance
    
    def _load_performance_baseline(self) -> Dict[str, Dict[str, float]]:
        """Load performance baseline from previous runs."""
        baseline_path = Path(__file__).parent / "performance_baseline.json"
        
        if baseline_path.exists():
            with open(baseline_path, 'r') as f:
                return json.load(f)
        else:
            return {}
    
    def _save_performance_baseline(self, performance_data: Dict[str, Dict[str, float]]):
        """Save current performance as new baseline."""
        baseline_path = Path(__file__).parent / "performance_baseline.json"
        
        with open(baseline_path, 'w') as f:
            json.dump(performance_data, f, indent=2)
    
    def run_performance_regression_tests(self) -> Dict[str, bool]:
        """Run performance regression tests."""
        test_cases = [
            ("small_graph", GraphGenerator.complete_graph(10)),
            ("medium_graph", GraphGenerator.long_path(50)),
            ("large_graph", GraphGenerator.star_graph(100))
        ]
        
        current_performance = {}
        regression_status = {}
        
        for test_name, graph in test_cases:
            source = list(graph.nodes())[0] if graph.nodes() else "dummy"
            
            # Measure performance
            times = []
            for _ in range(5):  # Run 5 times for average
                start_time = time.perf_counter()
                try:
                    result = run_implementation('python_bmssp', graph, source)
                    execution_time = time.perf_counter() - start_time
                    times.append(execution_time)
                except Exception:
                    times.append(float('inf'))
            
            avg_time = np.mean([t for t in times if t != float('inf')])
            current_performance[test_name] = {
                'avg_execution_time': avg_time,
                'min_execution_time': min(times),
                'max_execution_time': max(times)
            }
            
            # Compare with baseline
            if test_name in self.baseline_performance:
                baseline_time = self.baseline_performance[test_name]['avg_execution_time']
                performance_ratio = avg_time / baseline_time
                
                if performance_ratio > (1 + self.performance_tolerance):
                    print(f"Performance regression in {test_name}: "
                          f"{avg_time:.4f}s vs baseline {baseline_time:.4f}s "
                          f"({performance_ratio:.2f}x slower)")
                    regression_status[test_name] = False
                else:
                    regression_status[test_name] = True
            else:
                regression_status[test_name] = True  # No baseline
        
        # Update baseline if no regressions
        if all(regression_status.values()):
            self._save_performance_baseline(current_performance)
        
        return regression_status

def run_implementation(impl_name: str, graph: nx.DiGraph, source: str) -> TestResult:
    """Run a specific implementation and return results."""
    # This is a placeholder - actual implementation would depend on
    # how each language implementation is structured
    
    if impl_name == 'python_bmssp':
        return _run_python_bmssp(graph, source)
    elif impl_name == 'dijkstra_reference':
        return _run_dijkstra_reference(graph, source)
    else:
        raise ValueError(f"Unknown implementation: {impl_name}")

def _run_python_bmssp(graph: nx.DiGraph, source: str) -> TestResult:
    """Run Python BMSSP implementation."""
    try:
        # Import the actual Python implementation
        sys.path.append(str(Path(__file__).parent.parent.parent / "implementations" / "python"))
        from bmssp import run_sssp, Graph
        
        # Convert NetworkX graph to implementation format
        impl_graph = Graph()
        for u, v, data in graph.edges(data=True):
            weight = data.get('weight', 1)
            impl_graph.add_edge(u, v, weight)
        
        start_time = time.perf_counter()
        distances = run_sssp(impl_graph, source)
        execution_time = time.perf_counter() - start_time
        
        return TestResult(
            implementation='python_bmssp',
            test_case='test',
            distances=distances,
            execution_time=execution_time,
            memory_usage=0,
            vertices_processed=len([d for d in distances.values() if d < float('inf')])
        )
        
    except Exception as e:
        return TestResult(
            implementation='python_bmssp',
            test_case='test',
            distances={},
            execution_time=0,
            memory_usage=0,
            vertices_processed=0,
            error=str(e)
        )

def _run_dijkstra_reference(graph: nx.DiGraph, source: str) -> TestResult:
    """Run reference Dijkstra implementation."""
    try:
        start_time = time.perf_counter()
        
        if source not in graph.nodes():
            distances = {}
        else:
            distances = nx.single_source_shortest_path_length(
                graph, source, weight='weight'
            )
            # Add unreachable vertices
            for node in graph.nodes():
                if node not in distances:
                    distances[node] = float('inf')
        
        execution_time = time.perf_counter() - start_time
        
        return TestResult(
            implementation='dijkstra_reference',
            test_case='test',
            distances=distances,
            execution_time=execution_time,
            memory_usage=0,
            vertices_processed=len([d for d in distances.values() if d < float('inf')])
        )
        
    except Exception as e:
        return TestResult(
            implementation='dijkstra_reference',
            test_case='test',
            distances={},
            execution_time=0,
            memory_usage=0,
            vertices_processed=0,
            error=str(e)
        )

class ComprehensiveTestSuite:
    """Main test suite orchestrator."""
    
    def __init__(self):
        self.edge_case_tests = EdgeCaseTests()
        self.regression_tests = RegressionTests()
        self.cross_language_tests = CrossLanguageTests()
        self.performance_tests = PerformanceRegressionTests()
    
    def run_all_tests(self) -> Dict[str, Any]:
        """Run the complete test suite."""
        print("ðŸ§ª Running Comprehensive BMSSP Test Suite")
        print("=" * 60)
        
        results = {
            'edge_cases': {},
            'property_based': {},
            'regression': {},
            'cross_language': {},
            'performance': {},
            'summary': {}
        }
        
        # 1. Edge Case Tests
        print("\nðŸ” Running Edge Case Tests...")
        edge_case_results = self._run_edge_case_tests()
        results['edge_cases'] = edge_case_results
        
        # 2. Property-Based Tests
        print("\nðŸŽ² Running Property-Based Tests...")
        property_results = self._run_property_tests()
        results['property_based'] = property_results
        
        # 3. Regression Tests
        print("\nðŸ“Š Running Regression Tests...")
        regression_results = self.regression_tests.run_regression_tests()
        results['regression'] = regression_results
        
        # 4. Cross-Language Tests
        print("\nðŸŒ Running Cross-Language Consistency Tests...")
        cross_lang_results = self.cross_language_tests.run_cross_language_tests()
        results['cross_language'] = cross_lang_results
        
        # 5. Performance Regression Tests
        print("\nâš¡ Running Performance Regression Tests...")
        performance_results = self.performance_tests.run_performance_regression_tests()
        results['performance'] = performance_results
        
        # Generate summary
        results['summary'] = self._generate_test_summary(results)
        
        # Save results
        self._save_test_results(results)
        
        return results
    
    def _run_edge_case_tests(self) -> Dict[str, Any]:
        """Run all edge case tests."""
        results = {}
        
        for test_case in self.edge_case_tests.test_cases:
            print(f"   Testing {test_case.name}...")
            
            # Deserialize graph
            graph = nx.DiGraph()
            graph.add_nodes_from(test_case.graph_data['nodes'])
            for u, v, w in test_case.graph_data['edges']:
                graph.add_edge(u, v, weight=w)
            
            # Run test
            try:
                result = run_implementation('python_bmssp', graph, test_case.source)
                
                # Verify properties
                violations = PropertyTester.verify_distance_properties(
                    result.distances, graph, test_case.source
                )
                
                results[test_case.name] = {
                    'passed': len(violations) == 0 and result.error is None,
                    'violations': violations,
                    'error': result.error,
                    'reachable_count': len([d for d in result.distances.values() 
                                          if d < float('inf')]),
                    'execution_time': result.execution_time
                }
                
            except Exception as e:
                results[test_case.name] = {
                    'passed': test_case.expected_properties.get('expected_error', False),
                    'violations': [],
                    'error': str(e),
                    'reachable_count': 0,
                    'execution_time': 0
                }
        
        return results
    
    def _run_property_tests(self) -> Dict[str, Any]:
        """Run property-based tests."""
        results = {}
        
        try:
            # Run hypothesis tests
            print("   Testing random graph properties...")
            PropertyBasedTests.test_random_graph_properties()
            results['random_graphs'] = {'passed': True, 'error': None}
        except Exception as e:
            results['random_graphs'] = {'passed': False, 'error': str(e)}
        
        try:
            print("   Testing tree properties...")
            PropertyBasedTests.test_tree_properties()
            results['tree_properties'] = {'passed': True, 'error': None}
        except Exception as e:
            results['tree_properties'] = {'passed': False, 'error': str(e)}
        
        try:
            print("   Testing path properties...")
            PropertyBasedTests.test_path_properties()
            results['path_properties'] = {'passed': True, 'error': None}
        except Exception as e:
            results['path_properties'] = {'passed': False, 'error': str(e)}
        
        return results
    
    def _generate_test_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive test summary."""
        summary = {
            'total_tests': 0,
            'passed_tests': 0,
            'failed_tests': 0,
            'categories': {},
            'critical_failures': [],
            'warnings': []
        }
        
        # Edge case summary
        edge_passed = sum(1 for r in results['edge_cases'].values() if r.get('passed', False))
        edge_total = len(results['edge_cases'])
        summary['categories']['edge_cases'] = {
            'passed': edge_passed,
            'total': edge_total,
            'pass_rate': edge_passed / max(1, edge_total)
        }
        
        # Property-based summary
        prop_passed = sum(1 for r in results['property_based'].values() if r.get('passed', False))
        prop_total = len(results['property_based'])
        summary['categories']['property_based'] = {
            'passed': prop_passed,
            'total': prop_total,
            'pass_rate': prop_passed / max(1, prop_total)
        }
        
        # Regression summary
        reg_passed = sum(1 for r in results['regression'].values() if r)
        reg_total = len(results['regression'])
        summary['categories']['regression'] = {
            'passed': reg_passed,
            'total': reg_total,
            'pass_rate': reg_passed / max(1, reg_total)
        }
        
        # Cross-language summary
        cross_lang_results = results['cross_language']
        cross_lang_passed = 0
        cross_lang_total = 0
        
        for test_name, test_results in cross_lang_results.items():
            for comparison, passed in test_results.items():
                cross_lang_total += 1
                if passed:
                    cross_lang_passed += 1
        
        summary['categories']['cross_language'] = {
            'passed': cross_lang_passed,
            'total': cross_lang_total,
            'pass_rate': cross_lang_passed / max(1, cross_lang_total)
        }
        
        # Performance summary
        perf_passed = sum(1 for r in results['performance'].values() if r)
        perf_total = len(results['performance'])
        summary['categories']['performance'] = {
            'passed': perf_passed,
            'total': perf_total,
            'pass_rate': perf_passed / max(1, perf_total)
        }
        
        # Overall summary
        summary['total_tests'] = edge_total + prop_total + reg_total + cross_lang_total + perf_total
        summary['passed_tests'] = edge_passed + prop_passed + reg_passed + cross_lang_passed + perf_passed
        summary['failed_tests'] = summary['total_tests'] - summary['passed_tests']
        summary['overall_pass_rate'] = summary['passed_tests'] / max(1, summary['total_tests'])
        
        # Identify critical failures
        if summary['categories']['regression']['pass_rate'] < 1.0:
            summary['critical_failures'].append("Regression tests failed - algorithm behavior changed")
        
        if summary['categories']['cross_language']['pass_rate'] < 0.8:
            summary['critical_failures'].append("Cross-language consistency issues detected")
        
        if summary['categories']['performance']['pass_rate'] < 0.8:
            summary['warnings'].append("Performance regressions detected")
        
        return summary
    
    def _save_test_results(self, results: Dict[str, Any]):
        """Save test results to file."""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        results_file = Path(__file__).parent / f"test_results_{timestamp}.json"
        
        # Convert non-serializable objects
        serializable_results = json.loads(json.dumps(results, default=str))
        
        with open(results_file, 'w') as f:
            json.dump(serializable_results, f, indent=2)
        
        print(f"\nðŸ“„ Test results saved to {results_file}")

# Pytest integration
class TestBMSSPEdgeCases:
    """Pytest-compatible edge case tests."""
    
    @pytest.fixture
    def edge_cases(self):
        return EdgeCaseTests()
    
    def test_empty_graph(self, edge_cases):
        """Test empty graph handling."""
        empty_case = next(case for case in edge_cases.test_cases if case.name == "empty_graph")
        
        graph = nx.DiGraph()
        # Should handle gracefully or raise expected error
        try:
            result = run_implementation('python_bmssp', graph, empty_case.source)
            assert result.error is not None or len(result.distances) == 0
        except Exception:
            # Exception is acceptable for empty graph
            pass
    
    def test_single_vertex(self, edge_cases):
        """Test single vertex graph."""
        single_case = next(case for case in edge_cases.test_cases if case.name == "single_vertex")
        
        graph = nx.DiGraph()
        graph.add_node('A')
        
        result = run_implementation('python_bmssp', graph, 'A')
        assert result.error is None
        assert result.distances.get('A') == 0
    
    def test_disconnected_components(self, edge_cases):
        """Test disconnected graph components."""
        disc_case = next(case for case in edge_cases.test_cases if case.name == "disconnected_components")
        
        graph = GraphGenerator.disconnected_components()
        result = run_implementation('python_bmssp', graph, 'A')
        
        # Should reach A, B, C but not D, E, F
        assert result.distances.get('A') == 0
        assert result.distances.get('B', float('inf')) < float('inf')
        assert result.distances.get('C', float('inf')) < float('inf')
        # D, E, F may or may not be infinite depending on BMSSP behavior

def main():
    """Main function to run comprehensive test suite."""
    # Set up test environment
    random.seed(42)
    np.random.seed(42)
    
    # Run comprehensive test suite
    suite = ComprehensiveTestSuite()
    results = suite.run_all_tests()
    
    # Print summary
    print("\n" + "=" * 60)
    print("ðŸŽ¯ TEST SUITE SUMMARY")
    print("=" * 60)
    
    summary = results['summary']
    print(f"Overall: {summary['passed_tests']}/{summary['total_tests']} tests passed "
          f"({summary['overall_pass_rate']:.1%})")
    
    for category, stats in summary['categories'].items():
        print(f"{category}: {stats['passed']}/{stats['total']} "
              f"({stats['pass_rate']:.1%})")
    
    if summary['critical_failures']:
        print("\nðŸš¨ CRITICAL FAILURES:")
        for failure in summary['critical_failures']:
            print(f"   - {failure}")
    
    if summary['warnings']:
        print("\nâš ï¸  WARNINGS:")
        for warning in summary['warnings']:
            print(f"   - {warning}")
    
    # Exit with appropriate code
    if summary['critical_failures']:
        sys.exit(1)
    elif summary['overall_pass_rate'] < 0.9:
        print("\nâš ï¸  Test suite passed but with warnings")
        sys.exit(0)
    else:
        print("\nâœ… All tests passed successfully!")
        sys.exit(0)

if __name__ == "__main__":
    main()